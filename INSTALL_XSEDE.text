
Introduction
------------

This document describes how to install and configure the software that publishes information about an XSEDE
resource during the current pilot/prototype phase. This process does not conform to the XSEDE way of doing
things, but is a simple approach suitable for this phase.

If you have any questions or problems, please contact Warren Smith (you should have his email address).

Prerequisites
-------------

You will need he git version control software to install and update the software described here.

This software requires Python 2.6 or 2.7. If several different versions are available, instructions below will
describe how to specify which version this software should use.

To publish messages to the XSEDE AMQP messaging services, you must authenticate using either an X.509 certificate
or a username/password. Contact Warren Smith about the option you prefer so that he can configure that option
in the messaging services.

Setup
-----

This installation will result in the execution of 3 different monitoring workflows:

1. periodic gatherings of queue and system state
2. continuous monitoring of scheduling logs for job state updates
3. periodic gathering of module information

These components don't necessarily need to execute on the same server, but it is easier to maintain them if
they do. The requirements on the server/user corresponding to the workflows above are:

1. The command line programs of your batch scheduler must be executable. The TeraGrid CTSS kits directory must
be readable.
2. The batch scheduler log file or directory must be readable.
3. The module files must be readable.

In addition, if X.509 authentication is being used when communicating with the XSEDE messaging services, an
RSA key and X.509 certificate must be readable by the user used to run the monitoring workflows.

Installation
------------

This version of the software is installed by simply retrieving it using git. No compilation is necessary, but
there is some configuration to perform. 

Select a directory that will contain the two software directories that you will download. This document will
use $DIR to identify this directory.  If you are running the monitoring workflows on multiple systems and you
are not installing the software into a shared directory, you will need to install the software on each system.

As the user that will be used to run the monitoring workflows, execute:

    $ cd $DIR
    $ git clone git@bitbucket.org:wwsmith/mtk.git
    $ git clone git@bitbucket.org:wwsmith/ipf.git

The first git command downloads a Python messaging client and the second git command download an information
gathering and publishing framework.

Environment Configuration
-------------------------

The first configuration step is to edit the file /opt/ipf/libexec/env.sh. This script is used to establish the
environment when running monitoring workflows. You should edit the bottom of this file so that:

* the PYTHON environment variable points to a Python 2.6 or 2.7 interpreter
* the batch scheduler commands are in PATH
* the tgwhatami and tgwhereami commands are in PATH

This can often be accomplished by the appropriate 'module load ...' commands in that script.

Module Workflow Configuration
-----------------------------

The first and easiest workflow to configure is the modules workflow which is defined in either modules.json or
lmod.json. If you are using the Modules package, examine the modules.json workflow. If you are using the lmod
package, examine the lmod.json workflow.

The only parameters you may need to change in the workflow are the keyfile and certfile ones under
ssl_options. The default configuration assumes that you have an RSA key file in $DIR/ipf/etc/key.pem and an
X.509 certificate file in $DIR/ipf/etc/cert.pem. Assuming you want to authenticate to the XSEDE messaging
services using X.509, you can either link/copy your key and certificate files into $DIR/ipf/etc/key.pem and
$DIR/ipf/etc/cert.pem or you can modify this workflow file so that the values of keyfile and certfile are the
paths to your key and certificate files (the user running the workflow must be able to read these files). For
example, change them to:

          "keyfile": "/etc/grid-security/hostkey.pem",
          "certfile": "/etc/grid-security/hostcert.pem",

If you wish to use a username/password to authenticate, contact Warren Smith about how to configure that.

Testing the Modules Workflow
----------------------------

To test your modules workflow, execute:

    $ $DIR/bin/run_workflow.sh xsede/glue2/modules.json

or:

    $ $DIR/bin/run_workflow.sh xsede/glue2/lmod.json

The output should resemble:

    2013-05-30 15:27:05,309 - ipf.engine - INFO - starting workflow lmod
    2013-05-30 15:27:05,475 - ipf.publish.AmqpStep - INFO - step-3 - publishing representation ApplicationsOgfJson of Applications lonestar4.tacc.teragrid.org
    2013-05-30 15:27:05,566 - ipf.publish.FileStep - INFO - step-4 - writing representation ApplicationsOgfJson of Applications lonestar4.tacc.teragrid.org
    2013-05-30 15:27:06,336 - ipf.engine - INFO - workflow succeeded

If any of the steps fail, that will be reported and an error message and stack trace should appear. Typical
failures are caused by the execution environment not having specific commands available. See the Environment
Configuration section above.

This workflow describes your modules as a JSON document containing GLUE 2 Application Environment and
Application Handle objects. This document is published to the XSEDE messaging services in step-3 and is
written to a local file in step-4. You can examine this local file in $DIR/ipf/var/modules.json. If you see
any errors in gathering module information, please notify Warren Smith.

Compute Workflow Configuration
-----------------------------

The second workflow to configure is the compute workflow that gathers compute-related information about your
resource; primarily by interacting with your batch scheduling system. Look at the
$DIR/etc/workflow/xsede/glue2/*_compute.json workflows. There are several different workflows corresponding to
different batch scheduler and batch scheduler/resource manager combinations. Edit the workflow that best
matches your configuration and:

* Modify the Name, Place, Latitude, and Longitude under location to match your center, if necessary.
* Modify the core_kit_directory under the GramEndpointStep to be the absolute path to this directory on your
  system. This step fetches information about your Globus GRAM install from the TeraGrid CTSS kit registration
  files.
* If you modified keyfile and certfile in your modules workflow, make the same changes here under both
  AmqpStep steps.

This workflow generates two types of documents. The first type describes the current state of your resource,
doesn't contain sensitive information, and we expect to make available without authentication. The second type
describes the queue state of your resource, contains sensitive information (user names), and will only be made
available to authenticated XSEDE users.

The workflow publishes a JSON version of these two document types to the XSEDE messaging service. The workflow
also writes JSON and XML versions of these document types to local disk. These local files are used for
debugging and the XML versions can be read by the TeraGrid WS-MDS for publication via TeraGrid IIS mechanisms.

Testing the Compute Workflow
----------------------------

To test your compute workflow, execute the workflow file you edited above:

    $ $DIR/bin/run_workflow.sh xsede/glue2/???_compute.json

The output should resemble:

    2013-05-30 15:50:43,590 - ipf.engine - INFO - starting workflow sge_compute
    2013-05-30 15:50:45,403 - ipf.publish.AmqpStep - INFO - step-12 - publishing representation PrivateOgfJson of Private lonestar4.tacc.teragrid.org
    2013-05-30 15:50:45,626 - ipf.publish.FileStep - INFO - step-14 - writing representation PrivateOgfJson of Private lonestar4.tacc.teragrid.org
    2013-05-30 15:50:45,864 - ipf.publish.FileStep - INFO - step-16 - writing representation PrivateTeraGridXml of Private lonestar4.tacc.teragrid.org
    2013-05-30 15:50:45,878 - ipf.publish.AmqpStep - INFO - step-11 - publishing representation PublicOgfJson of Public lonestar4.tacc.teragrid.org
    2013-05-30 15:50:46,110 - ipf.publish.FileStep - INFO - step-13 - writing representation PublicOgfJson of Public lonestar4.tacc.teragrid.org
    2013-05-30 15:50:46,370 - ipf.publish.FileStep - INFO - step-15 - writing representation PublicTeraGridXml of Public lonestar4.tacc.teragrid.org
    2013-05-30 15:50:46,516 - ipf.engine - INFO - workflow succeeded

Again, typical failures are caused by the execution environment not having specific commands available. See
the Environment Configuration section above.

You can examine $DIR/ipf/var/public.json to determine if the description of your resource is accurate. You can
also exampine $DIR/ipf/var/private.json to determine if the description of the jobs being managed by your
resource is correct.

Activity Workflow Configuration
-------------------------------

The ???_compute.json workflow you edited above has a corresponding ???_activity.json workflow.  If you
modified keyfile and certfile in your modules workflow, make the same changes here.  This workflow will
typically use environment variables to find the location of the log file or directory of your batch scheduler,
so you should not need to make any other modifications.


Testing the Activity Workflow
-----------------------------

To test your activity workflow, execute the workflow file you edited above:

    $ $DIR/bin/run_workflow.sh xsede/glue2/???_activity.json

This is a long-running workflow and won't stop until you ctrl-C it. Let it run for a little while to make sure
that it is detecting job state changes.

The output should resemble:

    2013-05-30 16:04:26,030 - ipf.engine - INFO - starting workflow sge_activity
    2013-05-30 16:04:26,038 - glue2.sge.ComputingActivityUpdateStep - INFO - step-3 - running
    2013-05-30 16:04:26,038 - glue2.log - INFO - opening file 27-6930448 (/opt/sge6.2/default/common/reporting)
    2013-05-30 16:05:50,067 - glue2.log - INFO - reopening file 27-6930448 (/opt/sge6.2/default/common/reporting)
    2013-05-30 16:05:50,089 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226387.mhagy.lonestar4.tacc.teragrid.org
    2013-05-30 16:05:50,493 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226814.hhe.lonestar4.tacc.teragrid.org
    2013-05-30 16:06:12,109 - glue2.log - INFO - reopening file 27-6930448 (/opt/sge6.2/default/common/reporting)
    2013-05-30 16:06:12,361 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226867.maxfei.lonestar4.tacc.teragrid.org
    2013-05-30 16:06:12,380 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226868.maxfei.lonestar4.tacc.teragrid.org
    2013-05-30 16:06:12,407 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226788.rlc2489.lonestar4.tacc.teragrid.org
    2013-05-30 16:06:12,428 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226865.byaa705.lonestar4.tacc.teragrid.org
    2013-05-30 16:06:12,448 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226862.ylu.lonestar4.tacc.teragrid.org
    ...
    KeyboardInterrupt

Cron Configuration
------------------

Once you've tested your workflows, the recommended way to run them permanently is to create cron jobs in the user account where you have installed and tested the software. For example:

    */2 * * * * $DIR/ipf/bin/run_workflow_cron.sh xsede/glue2/???_compute.json
    */15 * * * * $DIR/ipf/bin/run_workflow_daemon.sh xsede/glue2/???_activity.json
    04 * * * * $DIR/ipf/bin/run_workflow_cron.sh xsede/glue2/modules.json

The first cron job will run the compute workflow to gather and publish the current state of your resource
every 2 minutes.

The second cron job will check that the long-running activity workflow is running every 15 minutes and will
restart it if it isn't (e.g. after a system reboot).

The third cron job gathers and publishes information about the modules available on your resource every hour
at 4 minutes past the hour.

Logging
-------

The workflows run out of cron will log their actions and errors as described above into log files in
$DIR/ipf/var. The log files have the same base name as the .json workflow description files.

Updating
--------

This software is under active development so it is expected that you will be asked to update it now and then
to grab bug fixes or new features.

To update your install, change to the ipf directory:

    $ cd $DIR/ipf

save the changes you've made (e.g. to env.sh and to the workflow description files):

    $ git stash

pull the updates from bitbucket:

    $ git pull

apply your saved changes to this updated version:

    $ git stash apply

and finally, kill the workflow daemon processes

    $ ps -eaf | grep run_workflow_daemon
    $ kill ...

and either restart it yourself:

    $ $DIR/ipf/bin/run_workflow_daemon.sh xsede/glue2/???_activity.json

or let cron restart it for you.
