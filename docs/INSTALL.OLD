ipf-glue2 %VER%-%REL% Install and Configure Instructions
========================================

Introduction
------------

This document describes how to install and configure the ipf framework with glue2 workflows software that publishes information about an XSEDE resource.

This installation will result in the execution of 5 different monitoring workflows:

1. periodic gathering of module information
2. periodic gatherings of queue and system state
3. continuous monitoring of scheduling logs for job state updates
4. periodic gathering of extended attribute module information (software)
5. periodic gathering of deployed service information (services)

To install ipf on machines that are part of multiple XSEDE resources
please first review the following Advanced Integration Options document:

https://www.ideals.illinois.edu/bitstream/handle/2142/99081/XSEDE_SP_Advanced_Integration_Options.pdf

Requirements
------------

The requirements on the server/user corresponding to the workflows above are:

1. The module files must be readable.
2. The command line programs of your batch scheduler must be executable.
3. The batch scheduler log file or directory must be readable. An important note is that the log file or directory does not need to be visibile to all users or all servers on your resource. Just to the server/user running the information gathering workflow.  
4. The batch scheduler must be logging at the right level of detail for the IPF code to be able to parse the events.  This has arisen during testing as a configuration problem for Torque--see the section: Configuring Torque Logging.
5. The module files must be readable
6. The service definition files must be readable, and in a (flat) services directory.  See section:  Configuring Service Files

In addition, if the XSEDE resources uses a key/certificate to authenticate when publishing, those files must be readable by the user account used to run the monitoring workflows.

Dependencies
------------

1. Python 2.6 or 2.7.
2. The Python amqp package. This package (python-amqp) is listed as a dependency in the ipf-xsede RPM.
3. For RPM installation, the python-setuptools package. This package is listed as a dependency in the ipf-xsede RPM.
4. Optionally, xdresourceid. If the xdresourceid program is not available, you will manually enter the name of your XSEDE resource into the configuration script.

RPM Installation
----------------

The default installation method is to install RPMs into standard locations (/usr/lib/pythonVERSION/site-packages/ipf, /etc/ipf, /var/ipf). If you wish to install elsewhere, see the tar.gz install instructions below.

Note: If you need an SD&I Development version, follow the instructions at https://software.xsede.org/development/repo/repoconfig.txt rather than step 1) below.

1) Configure trust in the XSEDE-Production repository

   a) Browse this directory and copy the link to your operating system's package:
      http://software.xsede.org/production/repo/repos/

   b) Install the package at the copied link:
      rpm -i http://software.xsede.org/production/repo/repos/XSEDE-Production-config.<OPERATING SYSTEM>.noarch.rpm
   You should see a warning similar to this:
      warning: XSEDE-Production-config.centos-5-1.noarch.rpm: Header V3 DSA signature: NOKEY, key ID 20423dbb
   This is a trust bootstrapping issue because until you complete this entire procedure, RPM doesn't trust the signer of this RPM.

   c) Lastly, configure RPM to trust XSEDE's signature (PGP key) installed by the above RPM:
      rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-XSEDE-Production

   SECURITY NOTE:
   If you need greater security assurance modify the above procedure as follows:
   a') Browse the package directory using https and review the server's HTTPS/TLS host certificate
   b') Download the package over https using your favorite secure download tool
   c') Verify the RPM before you install it using "rpm -Kv <package>"

2) Install ipf-xsede.

Note: The ipf-xsede rpm will automatically create an "xdinfo" account that will own the install and that will execute the workflows via sudo.

To install to the default location you can simply do:
   # yum install ipf-xsede

Next, follow the Configuration instructions below

tar.gz Installation
-------------------

If you wish to install this software in an alternate location or do not want to install from RPMs for whatever reason, this section describes how to install the software from .tgz files.

Note: If you need to install development versions, replace the URL prefixes 'http://software.xsede.org/production' with 'http://software.xsede.org/development' in the instructions below.

1) Create a normal user to run this software
   a) 'xdinfo' is the recommended username, but you can select a different one

2) Install the Python amqp package. You can 'yum install python-amqp', 'pip install amqp' (into the default location or into a Python virtualenv) or you can simply install from the .tgz:
   a) Download the .tgz file from https://pypi.python.org/pypi/amqp
   b) Untar the .tgz file into a convenient directory (perhaps the same directory you will install IPF into).
     Note: You may need to do this as root, depending on the install directory (e.g. /opt/). These files need to be readable by the user you created in step 1).

3) Install the XSEDE version of the Information Publishing Framework (ipf-xsede):
   a) Download the .tgz file from http://software.xsede.org/production/ipf/ipf-xsede/latest/
   b) Untar this file into your install directory. The result will be a directory hierarchy under $INSTALL_DIR/ipf-VERSION.

4) As the user created in step 1), configure the ipf_workflow script in $INSTALL_DIR/ipf-VERSION/ipf/bin
   a) Set PYTHON to be name of (or the full path to) the python interpreter you wish to use
   b) If you installed Python amqp into a non-default location, set AMQP_PATH to be the path where you installed it and uncomment this line and the following line that sets PYTHONPATH
   d) Run '$INSTALL_DIR/ipf-VERSION/ipf/bin/ipf_workflow sysinfo.json'. The workflow steps should run, but you may see failures related to host and/or site names.

Updating an ipf-xsede installation
----------------------------------

The json files that have been written by previous runs of ipf_configure_xsede would, in some previous versions of IPF get overwritten by subsequent runs of ipf_configure_xsede.  This is no longer the case--previous versions get backed up, not overwritten.  They will _not_ be erased by removing OR updating the package (nor will the service files copied to /etc/init.d be erased). 

It remains best practice to make a backup of /etc/ipf before upgrading.  The files should remain, but it is always best to back up working configurations.

To perform the update to the latest RPM distribution of ipf-xsede:
	1)  Backup /etc/ipf files
	2)  $ sudo yum update ipf-xsede
 	3)  If there are new workflows you need to configure, follow the 
	    configuration steps as outlined in the Configuration section below.

If you are updating an installation that was performed using the tar.gz distribution, it is recommended that you simply install the new version into a new directory.  Since everything (aside from the service files copied to /etc/init.d) is contained within a single directory, the new installation should not interfere with previous installations.  Note that if you do not make any changes to the service files copied to /etc/init.d, they will continue to use the older version.  Update the IPF_ETC_PATH, IPF_VAR_PATH, and PROGRAM (with full path to ipf_workflow program) to the values corresponding to your updated installation to utilize the updated version of ipf-xsede.

Configuration
-------------

To make configuration easier, an ipf_configure_xsede script is provided in the bin directory (in /usr/bin if you installed RPMs, otherwise in $INSTALL_DIR/ipf-VERSION/ipf/bin). This script will ask you questions and generate workflow definition files and example init files.  If you intend to publish module (or extended attribute module) information, you are advised to set the environment variable MODULEPATH to point to the location of the module files before running ipf_configure_xsede.  If you intend to publish the service workflow, you are advised to set SERVICEPATH to point to the location of the service definition files before running ipf_configure_xsede (more on this below).

* ipf_configure_xsede should be run as the user that will run the information gathering workflows

* ipf_configure_xsede will attempt to discover the XSEDE name of your resource using the xdresourceid program. If you have the xdresourceid program installed on your server, load it into your command line environment before running ipf_configure_xsede.

* The preferred way to authenticate is via an X.509 host certificate and key. You can place these files wherever you like (the configuration program will ask you for their location), but the default locations are /etc/grid-security/xdinfo-hostcert.pem and /etc/grid-security/xdinfo-hostkey.pem. These files must be readable by the user that runs the information gathering workflows.

* Submit an XSEDE ticket to authenticate your server to the XSEDE RabbitMQ services. If you will authenticate via X.509, include the output of 'openssl x509 -in path/to/cert.pem -nameopt RFC2253 -subject -noout' in your ticket. If you will authenticate via username and password, state that and someone will contact you.

Note: The xdinfo user as created by the ipf-xsede rpm installation has /bin/nologin set as its shell by default. This is because for most purposes, the xdinfo user doesn’t need an interactive shell. However, for some of the initial setup, it is easiest to use the xdinfo user with an interactive shell (so that certain environment variables like MODULEPATH can be discovered.)  Thus, it is recommended that the configuration steps are run after something like the following:
	$ sudo -u xdinfo -s /bin/bash --rcfile /etc/bashrc -i
	$ echo $MODULEPATH
	$ echo $SERVICEPATH

Execute:

$ ipf_configure_xsede

Follow the instructions and enter the requested information. If you encounter any errors or the script does not cover your situation, please submit an XSEDE ticket.

When the script exits, the etc/ipf/workflow/glue2/ directory will contain a set of files RESOURCE_NAME_*.json that describe the information gathering workflows you have configured and etc/ipf/init.d will contain ipf-RESOURCE_NAME_* files which are the init scripts you have configured.

As root, copy the etc/ipf/init.d/ipf-RESOURCE_NAME-* files into /etc/init.d. Your information gathering workflows can then be enabled, started, and stopped in the usual ways.

Configuring Torque Logging
--------------------------
It is necessary for Torque to log at the correct level in order for IPF to be able to parse the messages that it uses in determining state.  Furthermore, the logging level of Torque can be confusing, as it has both a log_level and a log_events (which is a bitmask).  The most important setting is that log_events should be set to 255.  This ensures that all types of events are logged.  You can check the setting on your Torque installation by using "qmgr".


Configuring Service Files
Service definition files have been part of Kit Registration and publishing since the days of TeraGrid. The Capability kits as such are deprecated in XSEDE, but we can continue to use the (slightly modified) service definition files for publishing through AMQP.
To do so, you should find the files that define services within your installed capability kits (NOTE: only <SERVICE>.conf files from your reg/service/ directories, not reg/kit.conf files), and copy them to a single directory. We recommend /etc/ipf/services/ (or $IPF/etc/services for a non RPM install), though you could place them anywhere. You must then set the environment variable SERVICEPATH to point to the directory where you have installed your service definition files. This will get encoded by ipf_configure_xsede into the periodic workflows, so that the services periodic workflow will be able to find the services files.

Unlike Teragrid Kit Registration, IPF will consider all filenames within
SERVICEPATH to be valid service definition files, except filenames that start
with ".".  In other words, both ExampleService.conf and ExampleService.noreg
would get published, but .ExampleService.conf will not.

There are some minor schema differences between the legacy service publishing
files and what the IPF Glue2 publishing services expect.  In general, if a
field from MDS publishing is no longer being used, it will not be harmful to
leave it in the service.conf file, but please take special note of how the
Name field has changed.

If you have not published CTSS Kit information through MDS/MDS stopgap, you
will have to create your service definition files from scratch.  The
information in the rest of this section is applicable both to those editing
existing service.conf files, and those creating them from scratch.

Each service.conf file needs to have a Name, Version, Endpoint, and
Capability.

The Name field in legacy MDS publishing referred to the human readable name of
the service, whereas in the new system, it refers to the
GLUE2 Primary protocol name supported by the service endpoint, as seen in the
table below.

Each service can have multiple capabilities, but each line is a key/value
pair, so to publish multiple capabilities for a service, have a line that
starts "Capability = " for each value.  (see example below).  Note:  in MDS
publishing, separate services were published for striped/non-striped GridFTP
servers, even if the endpoint URL was the same.  In IPF publishing, the
Capability fields describe what the service at the endpoint URL supports.

Valid capability values can be seen in the table below.  Please edit your
service.conf files to include appropriate Capability values.

A key/value pair for SupportStatus in your service.conf file will override the
default, which is the support status of your service as published in RDR.

_________________________________________________________________________
A table of valid Name,version and capability values:

Name			Version			Capability
org.globus.gridftp 	{5,6}.y.z 		data.transfer.striped
						data.transfer.nonstriped

org.globus.gram		{5,6}.y.z		executionmanagement.jobdescription
						executionmanagement.jobexecution
						executionmanagement.jobmanager

org.globus.openssh 	5.y.z 			login.remoteshell
						login.remoteshell.gsi
eu.unicore.tsf 		{6,7}.y.z 		executionmanagement.jobdescription
						executionmanagement.jobexecution
						executionmanagement.jobmanager

eu.unicore.bes 		{6,7}.y.z 		executionmanagement.jobdescription
						executionmanagement.jobexecution
						executionmanagement.jobmanager

eu.unicore.reg 		{6,7}.y.z 		Information.publication

org.xsede.gpfs 		3.5 			data.access.flatfiles

org.xsede.genesisII	2.y.z 			data.access.flatfiles
						data.naming.resolver
org.xsede.mds­stopgap	1.2			information.publication

______________________________________________________________________________
Sample Service publishing file:

#%Service1.0################################################################### ##
## serviceinfofiles/org.globus.gridftp-6.0.1.conf
##

Name = org.globus.gridftp
Version = 6.0.1
Endpoint = gsiftp://$GRIDFTP_PUBLIC_HOSTNAME:2811/
Extensions.go_transfer_xsede_endpoint_name = "default"
Capability = data.transfer.striped
Capability = data.transfer.nonstriped
SupportStatus = testing



Best Practices for Software publishing  (Modules Files)
------------------------------------------------------

XSEDE has two levels of software information that are published:  the software
catalog at the Portal (which is used for global, static software information),
and the dynamic software information published to Information Services using
the IPF pub/sub publishing framework.

The information published via the IPF pub/sub publishing framework is meant to
be information that is local to the installation--if there is global
information it should be published at the XSEDE Portal.

The XSEDE IPF pub/sub publishing framework attempts to make intelligent
inferences from the system installed modules files when it publishes software
information.  There are some easy ways, however, to add information to your
module files that will enhance/override the information otherwise published.

The Modules workflows for IPF search your MODULEPATH, and infer fields such as
name and version from the directory structure/naming conventions of the module
file layout.  Depending on the exact workflow steps, fields such as
Description may be blank, or inferred from the stdout/stderr text of the
module.  However, the following fields can always be added to a module file to
be published:

Description:
URL:
Category:
Keywords:
SupportStatus:
SupportContact:

Each field is a key: value pair.  The IPF workflows are searching the whole
text of each module file for these fields--they may be placed in a
module-whatis line, or in a comment, and IPF will still read them.

Example:

#%Module1.0#####################################################################
##
## modulefiles/xdresourceid/1.0
##
module-whatis "Description: XSEDE Resource Identifier Tool"
module-whatis "URL: http://software.xsede.org/development/xdresourceid/"
module-whatis "Category: System tools"
module-whatis "Keywords: information"
module-whatis "SupportStatus: testing"

However, IPF would read these just as well if they were:

#%Module1.0#####################################################################
##
## modulefiles/xdresourceid/1.0
##
#module-whatis "Description: XSEDE Resource Identifier Tool"
# "URL: http://software.xsede.org/development/xdresourceid/"
# Random text that is irrelevant "Category: System tools"
# module-whatis "Keywords: information"
module-whatis "SupportStatus: testing"

To this end, XSEDE recommends that you add these fields to relevant module
files.  The decision on whether to include them as module-whatis lines (and
therefore visible as such to local users) or to include them as comments is
left to the site admins.




Testing
-------
In IPF 1.3 and higher, lmod/modules workflows are deprecated, and extmodules
should be used for all software publishing.

1) To test the extended attribute modules workflow, execute:

    # service ipf-RESOURCE_NAME-glue2-extmodules start

This init script starts a workflow that periodically gathers (every hour by default) and publishes module information containing extended attributes.

The log file is in /var/ipf/RESOURCE_NAME_modules.log (or $INSTALL_DIR/ipf/var/ipf/RESOURCE_NAME_extmodules.log) and should contain messages resembling:

    2013-05-30 15:27:05,309 - ipf.engine - INFO - starting workflow extmodules
    2013-05-30 15:27:05,475 - ipf.publish.AmqpStep - INFO - step-3 - publishing representation ApplicationsOgfJson of Applications lonestar4.tacc.teragrid.org
    2013-05-30 15:27:05,566 - ipf.publish.FileStep - INFO - step-4 - writing representation ApplicationsOgfJson of Applications lonestar4.tacc.teragrid.org
    2013-05-30 15:27:06,336 - ipf.engine - INFO - workflow succeeded

If any of the steps fail, that will be reported and an error message and stack trace should appear. Typical failures are caused by the environment not having specific variables or commands available.

This workflow describes your modules as a JSON document containing GLUE v2.0 Application Environment and Application Handle objects. This document is published to the XSEDE messaging services in step-3 and is written to a local file in step-4. You can examine this local file in /var/ipf/RESOURCE_NAME_apps.json. If you see any errors in gathering module information, please submit an XSEDE ticket to SD&I.


2) To test the compute workflow, execute:

    # service ipf-RESOURCE_NAME-glue2-compute start

This init script starts a workflow that periodically gathers (every minute by default) and publishes information about your compute resource. This workflow generates two types of documents. The first type describes the current state of your resource. This document doesn't contain sensitive information and XSEDE makes it available without authentication. The second type describes the queue state of your resource, contains sensitive information (user names), and will only be made available to authenticated XSEDE users.

The log file is in /var/ipf/RESOURCE_NAME_compute.log (or $INSTALL_DIR/ipf/var/ipf/RESOURCE_NAME_compute.log) and should contain messages resembling:

    2013-05-30 15:50:43,590 - ipf.engine - INFO - starting workflow sge_compute
    2013-05-30 15:50:45,403 - ipf.publish.AmqpStep - INFO - step-12 - publishing representation PrivateOgfJson of Private stampede.tacc.teragrid.org
    2013-05-30 15:50:45,626 - ipf.publish.FileStep - INFO - step-14 - writing representation PrivateOgfJson of Private stampede.tacc.teragrid.org
    2013-05-30 15:50:45,878 - ipf.publish.AmqpStep - INFO - step-11 - publishing representation PublicOgfJson of Public stampede.tacc.teragrid.org
    2013-05-30 15:50:46,110 - ipf.publish.FileStep - INFO - step-13 - writing representation PublicOgfJson of Public stampede.tacc.teragrid.org
    2013-05-30 15:50:46,516 - ipf.engine - INFO - workflow succeeded

Typical failures are caused by the execution environment not having specific commands available. Review the environment setup in the init script.

You can examine /var/ipf/RESOURCE_NAME_compute.json (or $INSTALL_DIR/ipf/var/ipf/RESOURCE_NAME_compute.json) to determine if the description of your resource is accurate. You can also exampine /var/ipf/RESOURCE_NAME_activities.json ( or $INSTALL_DIR/ipf/var/ipf/RESOURCE_NAME_activities.json) to determine if the description of the jobs being managed by your resource is correct.

3) To test the activity workflow, execute:

    # service ipf-RESOURCE_NAME-glue2-activity start

This init script starts a long-running workflow that watches your scheduler log files and publishes information about jobs as they are submitted and change state.

The log file is in /var/ipf/RESOURCE_NAME_activity.log (or $INSTALL_DIR/ipf/var/ipf/RESOURCE_NAME_activity.log) and should contain messages resembling:

    2013-05-30 16:04:26,030 - ipf.engine - INFO - starting workflow pbs_activity
    2013-05-30 16:04:26,038 - glue2.pbs.ComputingActivityUpdateStep - INFO - step-3 - running
    2013-05-30 16:04:26,038 - glue2.log - INFO - opening file 27-6930448 (/opt/pbs6.2/default/common/reporting)
    2013-05-30 16:05:50,067 - glue2.log - INFO - reopening file 27-6930448 (/opt/pbs6.2/default/common/reporting)
    2013-05-30 16:05:50,089 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226387.user.resource.xsede.org
    2013-05-30 16:05:50,493 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226814.user.resource.xsede.org
    2013-05-30 16:06:12,109 - glue2.log - INFO - reopening file 27-6930448 (/opt/pbs6.2/default/common/reporting)
    2013-05-30 16:06:12,361 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226867.user.resource.xsede.org
    2013-05-30 16:06:12,380 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226868.user.resource.xsede.org
    2013-05-30 16:06:12,407 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226788.user.resource.xsede.org
    2013-05-30 16:06:12,428 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226865.user.resource.xsede.org
    2013-05-30 16:06:12,448 - ipf.publish.AmqpStep - INFO - step-4 - publishing representation ComputingActivityOgfJson of ComputingActivity 1226862.user.resource.xsede.org
    ...

You can look at the activity information published in /var/ipf/RESOURCE_NAME_activity.json (or $INSTALL_DIR/ipf/var/ipf/RESOURCE_NAME_activity.json). This file contains the sequence of activity JSON documents published.

4) To test the Abstract Service (services) workflow, execute:

    # service ipf-RESOURCE_NAME-glue2-services start

This init script starts a workflow that periodically gathers (every hour by default) and publishes service information from the service definition files (that used to be included in TeraGrid Kit publishing).

The log file is in /var/ipf/RESOURCE_NAME_services.log (or $INSTALL_DIR/ipf/var/ipf/RESOURCE_NAME_services.log) and should contain messages resembling:

    2013-05-30 15:27:05,309 - ipf.engine - INFO - starting workflow s
    2013-05-30 15:27:05,475 - ipf.publish.AmqpStep - INFO - step-3 - publishing representation ASOgfJson of AbstractServices lonestar4.tacc.teragrid.org
    2013-05-30 15:27:05,566 - ipf.publish.FileStep - INFO - step-4 - writing representation ASOgfJson of AbstractServices lonestar4.tacc.teragrid.org
    2013-05-30 15:27:06,336 - ipf.engine - INFO - workflow succeeded

If any of the steps fail, that will be reported and an error message and stack trace should appear. Typical failures are caused by the environment not having specific variables or commands available.

This workflow describes your modules as a JSON document containing GLUE v2.0 Application Environment and Application Handle objects. This document is published to the XSEDE messaging services in step-3 and is written to a local file in step-4. You can examine this local file in /var/ipf/RESOURCE_NAME_apps.json. If you see any errors in gathering module information, please submit an XSEDE ticket to SD&I.

Log File Management
-------------------

The log files described above will grow over time. The logs are only needed for debugging, so they can be deleted whenever you wish. logrotate is a convenient tool for creating archival log files and limiting the amount of files kept. One note is that the ipf workflows keep the log files open while they run, so you should either use the copytruncate option or have a post-rotate statement to restart the corresponding ipf service. 
